1)
import boto3
from botocore.exceptions import ClientError

def restore_glacier_objects(bucket_name, object_keys, days=90, tier='Expedited'):
    """
    Restore S3 Glacier objects.

    :param bucket_name: Name of the S3 bucket
    :param object_keys: List of object keys to restore
    :param days: Number of days to keep the restored objects
    :param tier: Retrieval tier ('Expedited', 'Standard', 'Bulk')
    """
    s3 = boto3.client('s3')
    
    for key in object_keys:
        try:
            # Initiate the restore request
            response = s3.restore_object(
                Bucket=bucket_name,
                Key=key,
                RestoreRequest={
                    'Days': days,
                    'GlacierJobParameters': {
                        'Tier': tier
                    }
                }
            )
            print(f'Restore request for {key} initiated: {response}')
        except ClientError as e:
            if e.response['Error']['Code'] == 'RestoreAlreadyInProgress':
                print(f'Restore already in progress for {key}')
            else:
                print(f'Error restoring {key}: {e}')

if __name__ == "__main__":
    bucket_name = 'your-bucket-name'
    object_keys = [
        'object-key-1',
        'object-key-2',
        # Add more object keys as needed
    ]

    restore_glacier_objects(bucket_name, object_keys)
2)==============================================================
For restoring a large number of objects (e.g., 1000+)


import boto3
import time
from botocore.exceptions import ClientError

def restore_glacier_objects(bucket_name, object_keys, days=90, tier='Expedited', batch_size=100):
    """
    Restore S3 Glacier objects in batches.

    :param bucket_name: Name of the S3 bucket
    :param object_keys: List of object keys to restore
    :param days: Number of days to keep the restored objects
    :param tier: Retrieval tier ('Expedited', 'Standard', 'Bulk')
    :param batch_size: Number of objects to restore in each batch
    """
    s3 = boto3.client('s3')
    
    for i in range(0, len(object_keys), batch_size):
        batch = object_keys[i:i + batch_size]
        for key in batch:
            success = False
            retries = 0
            while not success and retries < 5:
                try:
                    # Initiate the restore request
                    response = s3.restore_object(
                        Bucket=bucket_name,
                        Key=key,
                        RestoreRequest={
                            'Days': days,
                            'GlacierJobParameters': {
                                'Tier': tier
                            }
                        }
                    )
                    print(f'Restore request for {key} initiated: {response}')
                    success = True
                except ClientError as e:
                    if e.response['Error']['Code'] == 'RestoreAlreadyInProgress':
                        print(f'Restore already in progress for {key}')
                        success = True
                    else:
                        retries += 1
                        wait_time = 2 ** retries  # Exponential backoff
                        print(f'Error restoring {key}: {e}. Retrying in {wait_time} seconds...')
                        time.sleep(wait_time)

if __name__ == "__main__":
    bucket_name = 'your-bucket-name'
    object_keys = [
        'object-key-1',
        'object-key-2',
        # Add more object keys as needed
        # ...
        'object-key-1000'
    ]

    restore_glacier_objects(bucket_name, object_keys)
3)========================================================
Certainly! To find and restore Glacier objects within a specific S3 bucket, you can use the list_objects_v2 method to list all objects in the bucket, check their storage class, and then apply the restore operation if they are in Glacier. 

import boto3
import time
from botocore.exceptions import ClientError

def list_glacier_objects(bucket_name):
    """
    List all objects in the bucket that are stored in Glacier.

    :param bucket_name: Name of the S3 bucket
    :return: List of Glacier object keys
    """
    s3 = boto3.client('s3')
    glacier_objects = []
    paginator = s3.get_paginator('list_objects_v2')

    for page in paginator.paginate(Bucket=bucket_name):
        for obj in page.get('Contents', []):
            head_response = s3.head_object(Bucket=bucket_name, Key=obj['Key'])
            if head_response['StorageClass'] == 'GLACIER':
                glacier_objects.append(obj['Key'])
    
    return glacier_objects

def restore_glacier_objects(bucket_name, object_keys, days=90, tier='Expedited', batch_size=100):
    """
    Restore S3 Glacier objects in batches.

    :param bucket_name: Name of the S3 bucket
    :param object_keys: List of object keys to restore
    :param days: Number of days to keep the restored objects
    :param tier: Retrieval tier ('Expedited', 'Standard', 'Bulk')
    :param batch_size: Number of objects to restore in each batch
    """
    s3 = boto3.client('s3')
    
    for i in range(0, len(object_keys), batch_size):
        batch = object_keys[i:i + batch_size]
        for key in batch:
            success = False
            retries = 0
            while not success and retries < 5:
                try:
                    # Initiate the restore request
                    response = s3.restore_object(
                        Bucket=bucket_name,
                        Key=key,
                        RestoreRequest={
                            'Days': days,
                            'GlacierJobParameters': {
                                'Tier': tier
                            }
                        }
                    )
                    print(f'Restore request for {key} initiated: {response}')
                    success = True
                except ClientError as e:
                    if e.response['Error']['Code'] == 'RestoreAlreadyInProgress':
                        print(f'Restore already in progress for {key}')
                        success = True
                    else:
                        retries += 1
                        wait_time = 2 ** retries  # Exponential backoff
                        print(f'Error restoring {key}: {e}. Retrying in {wait_time} seconds...')
                        time.sleep(wait_time)

if __name__ == "__main__":
    bucket_name = 'your-bucket-name'

    # List Glacier objects
    glacier_objects = list_glacier_objects(bucket_name)
    print(f'Found {len(glacier_objects)} Glacier objects.')

    # Restore Glacier objects
    if glacier_objects:
        restore_glacier_objects(bucket_name, glacier_objects)
