apiVersion: apps/v1
kind: Deployment
metadata:
  name: myweb
  namespace: default
spec:
  replicas: 10
  selector:
    matchLabels:
      app: myweb
  template:
    metadata:
      labels:
        app: myweb
    spec:
      containers:
        - name: myweb-container
          image: <your-app-image>
          volumeMounts:
            - name: myweb-logs
              mountPath: /opt/mypath
              subPathExpr: $(POD_NAME)
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
      volumes:
        - name: myweb-logs
          hostPath:
            path: /var/myweb-logs
            type: DirectoryOrCreate
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-backup-truncator
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: log-backup-truncator
  template:
    metadata:
      labels:
        app: log-backup-truncator
    spec:
      containers:
        - name: backup-logs
          image: amazonlinux:2
          command:
            - /bin/sh
            - -c
            - |
              yum install -y gzip || true
              while true; do
                TODAY=$(date +%F)
                HOSTNAME=$(hostname)
                BACKUP_DIR=/mnt/efs/mybackups/${TODAY}/${HOSTNAME}
                mkdir -p ${BACKUP_DIR}

                echo "[INFO] Backing up logs to ${BACKUP_DIR}"

                find /var/myweb-logs -type f -name "*.log" | while read file; do
                  podname=$(basename $(dirname "$file"))
                  filename=$(basename "$file")
                  gzip -c "$file" > "${BACKUP_DIR}/${podname}-${filename}.gz"
                  echo "[INFO] Truncating $file"
                  truncate -s 0 "$file"
                done

                echo "[INFO] Sleeping for 24h"
                sleep 86400
              done
          volumeMounts:
            - name: myweb-logs
              mountPath: /var/myweb-logs
            - name: efs-backup
              mountPath: /mnt/efs
      restartPolicy: Always
      volumes:
        - name: myweb-logs
          hostPath:
            path: /var/myweb-logs
            type: Directory
        - name: efs-backup
          persistentVolumeClaim:
            claimName: efs-backup-pvc
---
LOGS_FOUND=$(find /var/myweb-logs -type f -name "*.log" | wc -l)
if [ "$LOGS_FOUND" -eq 0 ]; then
  echo "[INFO] No logs found on this node. Sleeping for 24h."
  sleep 86400
  continue
fi
---

#fluentbit config
[SERVICE]
    Flush         1
    Log_Level     info
    Parsers_File  parsers.conf
    Daemon        Off

@include inputs.conf
@include outputs.conf
---

[INPUT]
    Name              tail
    Path              /var/myweb-logs/*/*.log
    Tag               myweb.*
    DB                /var/log/flb_myweb.db
    Read_from_Head    true
    Refresh_Interval  10
    Rotate_Wait       30
    Skip_Long_Lines   On
    Buffer_Chunk_Size 512KB
    Buffer_Max_Size   5MB
---
[OUTPUT]
    Name            cloudwatch_logs
    Match           myweb.*
    region          <your-aws-region>
    log_group_name  /eks/myweb-app
    log_stream_prefix from-
    auto_create_group true
---
#fluentbit daemonset

volumes:
  - name: myweb-logs
    hostPath:
      path: /var/myweb-logs
      type: Directory

volumeMounts:
  - name: myweb-logs
    mountPath: /var/myweb-logs
---
[MULTILINE_PARSER]
    Name              multiline_java
    Type              regex
    Flush_Interval    2
    Key_Content       log
    Rule              "start_state"  "^[0-9]{4}-[0-9]{2}-[0-9]{2}.*"  "cont"
    Rule              "cont"         "^\\s+at .*"                    "cont"
    Rule              "cont"         "^[a-zA-Z].*"                   "cont"
---
[INPUT]
    Name              tail
    Path              /var/myweb-logs/*/*.log
    Tag               myweb.*
    Multiline.parser  multiline_java
    DB                /buffers/flb_myweb.db
    Read_from_Head    true
---
[MULTILINE_PARSER]
    Name              multiline_generic
    Type              regex
    Flush_Interval    2
    Key_Content       log
    Rule              "start_state"  "^\d{4}-\d{2}-\d{2}.*"  "cont"
    Rule              "cont"         "^\s+.*"                "cont"
    Rule              "cont"         "^Caused by:.*"         "cont"
    Rule              "cont"         "^\tat .*"              "cont"
---
#!/bin/bash

SRC="/var/myweb-logs"
DEST="/mnt/efs/myweb-backup/$(date +%F)"
mkdir -p "$DEST"

# Identify latest modified folder (most likely the active pod/container)
ACTIVE_FOLDER=$(find "$SRC" -maxdepth 1 -mindepth 1 -type d -printf "%T@ %p\n" | sort -nr | head -n1 | awk '{print $2}')

echo "Skipping active log folder: $ACTIVE_FOLDER"

# Copy all except the active folder
for folder in "$SRC"/*; do
    if [[ "$folder" != "$ACTIVE_FOLDER" ]]; then
        cp -r "$folder" "$DEST"/
    fi
done

---
#!/bin/bash

input_file="output.log"
current_path=""
java_containers=()

while IFS= read -r line; do
  # If line starts with /var/log, treat it as a new log file path
  if [[ "$line" == /var/log/pega-logs/* ]]; then
    current_path="$line"
  fi

  # If line contains 'java', record the associated path
  if echo "$line" | grep -q "java"; then
    container_path=$(echo "$current_path" | cut -d'/' -f1-5)  # Adjust as needed
    java_containers+=("$container_path")
  fi
done < "$input_file"

# Print unique container paths
printf "%s\n" "${java_containers[@]}" | sort -u
readarray -t ufiles < <(printf "%s\n" "${java_containers[@]}" | sort -u)

# Step 3: Copy the entire directory for each container to EFS
for path in "${ufiles[@]}"; do
  echo "Copying $path to $efs_dest"
  cp -r "$path" "$efs_dest"/
done

for path in "${ufiles[@]}"; do
  echo "Truncating log files in: $path"
  find "$path" -type f -name "*.log" -exec truncate -s 0 {} \;
done
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: pega-used-log-detector
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: pega-used-log-detector
  template:
    metadata:
      labels:
        app: pega-used-log-detector
    spec:
      hostPID: true
      containers:
      - name: detector
        image: amazonlinux:2
        command: ["/bin/bash", "-c", "chmod +x /opt/script/check-used-folders.sh && /opt/script/check-used-folders.sh && sleep 3600"]
        volumeMounts:
        - name: pega-logs
          mountPath: /var/log/pega-logs
        - name: host-proc
          mountPath: /host_proc
          readOnly: true
        - name: script
          mountPath: /opt/script
      restartPolicy: Always
      volumes:
      - name: pega-logs
        hostPath:
          path: /var/log/pega-logs
      - name: host-proc
        hostPath:
          path: /proc
      - name: script
        configMap:
          name: pega-used-folder-script
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pega-used-folder-script
  namespace: kube-system
data:
  check-used-folders.sh: |
    #!/bin/bash
    LOG_BASE="/var/log/pega-logs"
    PROC_BASE="/host_proc"
    used_folders=()

    for pid in $(ls $PROC_BASE | grep -E '^[0-9]+$'); do
      for fd in "$PROC_BASE/$pid/fd"/*; do
        target=$(readlink "$fd" 2>/dev/null)
        if [[ "$target" == "$LOG_BASE"* ]]; then
          folder=$(echo "$target" | cut -d'/' -f1-5)
          used_folders+=("$folder")
        fi
      done
    done

    readarray -t unique_folders < <(printf "%s\n" "${used_folders[@]}" | sort -u)
    echo "Folders currently used by pods:"
    printf "%s\n" "${unique_folders[@]}"


